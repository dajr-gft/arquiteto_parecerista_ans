# Su√≠te de Testes Unit√°rios - Architecture Domain ANS Agent

## üìã Vis√£o Geral

Esta √© a su√≠te de testes unit√°rios para o agente **Architecture Domain ANS**, desenvolvido com Google Agent Development Kit (ADK). A su√≠te foi projetada para garantir qualidade, confiabilidade e cobertura m√≠nima de 85% do c√≥digo.

## üèóÔ∏è Estrutura da Su√≠te

```
bv_ans/testes/unit_tests/
‚îú‚îÄ‚îÄ __init__.py                 # Inicializa√ß√£o do pacote de testes
‚îú‚îÄ‚îÄ conftest.py                 # Fixtures compartilhadas e configura√ß√£o pytest
‚îú‚îÄ‚îÄ test_agent_core.py          # Testes do n√∫cleo do agente
‚îú‚îÄ‚îÄ test_tools.py               # Testes das ferramentas (tools)
‚îî‚îÄ‚îÄ test_prompts.py             # Testes dos prompts e templates
```

### Descri√ß√£o dos Arquivos

#### `conftest.py`
Cont√©m fixtures reutiliz√°veis e configura√ß√£o de ambiente de teste:
- **Fixtures de Dados**: CNPJs v√°lidos/inv√°lidos, IDs de API, emails
- **Fixtures OneTrust**: Dados de fornecedores encontrados/n√£o encontrados
- **Fixtures CMDB**: Dados de servi√ßos com diferentes direcionadores
- **Fixtures Hist√≥rico**: Pareceres anteriores para an√°lise
- **Fixtures Sugest√£o**: Dados de requisi√ß√£o com diferentes cen√°rios
- **Fixtures Registro**: Dados completos/incompletos para registro de pareceres

#### `test_agent_core.py`
Testes focados no n√∫cleo do agente:
- ‚úÖ Inicializa√ß√£o e configura√ß√£o do agente
- ‚úÖ Configura√ß√£o de modelo (Gemini 3 Pro)
- ‚úÖ Configura√ß√£o de ferramentas (7 tools esperadas)
- ‚úÖ Configura√ß√£o de planner e thinking
- ‚úÖ Vari√°veis de ambiente e integra√ß√£o Vertex AI
- ‚úÖ Logging e constantes
- ‚úÖ Valida√ß√£o de metadados e descri√ß√µes

**Total**: 40+ testes organizados em 13 classes

#### `test_tools.py`
Testes abrangentes para todas as ferramentas do agente:
- ‚úÖ **integrar_onetrust**: Consulta de fornecedores, normaliza√ß√£o de CNPJ, c√°lculo de vencimento
- ‚úÖ **consultar_cmdb**: Consulta de servi√ßos, direcionadores, metadados
- ‚úÖ **carregar_insumos**: Carregamento de hist√≥rico, normaliza√ß√£o, padr√µes identificados
- ‚úÖ **sugerir_parecer**: L√≥gica de sugest√£o, crit√©rios aplicados, score de confian√ßa
- ‚úÖ **registrar_parecer**: Valida√ß√£o de campos, gera√ß√£o de ID, persist√™ncia
- ‚úÖ **capturar_vencimento**: Verifica√ß√£o de exist√™ncia e importa√ß√£o
- ‚úÖ **carregar_ressalvas**: Verifica√ß√£o de exist√™ncia e importa√ß√£o

**Total**: 50+ testes organizados em 7 classes

#### `test_prompts.py`
Testes dos prompts e templates do sistema:
- ‚úÖ Estrutura e exist√™ncia de prompts
- ‚úÖ Conte√∫do e keywords importantes
- ‚úÖ Formata√ß√£o e qualidade do texto
- ‚úÖ Prompts otimizados vs. base
- ‚úÖ Consist√™ncia entre vers√µes
- ‚úÖ Tom profissional e clareza
- ‚úÖ Defini√ß√£o de outputs e contexto

**Total**: 35+ testes organizados em 9 classes

## üöÄ Como Executar os Testes

### Pr√©-requisitos

1. **Instalar depend√™ncias**:
```powershell
pip install -r requirements.txt
```

Isso instalar√°:
- `pytest>=8.3.5` - Framework de testes
- `pytest-asyncio>=0.26.0` - Suporte para testes ass√≠ncronos
- `pytest-cov>=6.0.0` - Cobertura de c√≥digo
- `pytest-mock>=3.14.0` - Mocking avan√ßado

### Executar Todos os Testes Unit√°rios

```powershell
pytest bv_ans/testes/unit_tests/ -v
```

### Executar com Relat√≥rio de Cobertura

```powershell
pytest bv_ans/testes/unit_tests/ -v --cov=architecture_domain_ans --cov-report=term-missing
```

### Executar com Relat√≥rio HTML

```powershell
pytest bv_ans/testes/unit_tests/ -v --cov=architecture_domain_ans --cov-report=html
```

O relat√≥rio HTML ser√° gerado em `htmlcov/index.html`.

### Executar Arquivo Espec√≠fico

```powershell
# Apenas testes do agente
pytest bv_ans/testes/unit_tests/test_agent_core.py -v

# Apenas testes de tools
pytest bv_ans/testes/unit_tests/test_tools.py -v

# Apenas testes de prompts
pytest bv_ans/testes/unit_tests/test_prompts.py -v
```

### Executar Teste Espec√≠fico

```powershell
pytest bv_ans/testes/unit_tests/test_agent_core.py::TestAgentInitialization::test_agent_has_correct_model -v
```

### Executar com Marcadores

```powershell
# Apenas testes r√°pidos
pytest bv_ans/testes/unit_tests/ -v -m "not slow"

# Apenas testes unit√°rios
pytest bv_ans/testes/unit_tests/ -v -m unit
```

## üìä Cobertura de C√≥digo

### Meta de Cobertura

**M√≠nimo obrigat√≥rio**: 85%

A configura√ß√£o no `pytest.ini` inclui `--cov-fail-under=85`, o que significa que os testes falhar√£o se a cobertura for inferior a 85%.

### Verificar Cobertura Atual

```powershell
pytest bv_ans/testes/unit_tests/ --cov=architecture_domain_ans --cov-report=term-missing
```

### Interpretar Relat√≥rio de Cobertura

O relat√≥rio mostrar√°:
- **Stmts**: N√∫mero total de statements
- **Miss**: Statements n√£o cobertos
- **Cover**: Percentual de cobertura
- **Missing**: Linhas espec√≠ficas n√£o cobertas

Exemplo:
```
Name                                    Stmts   Miss  Cover   Missing
---------------------------------------------------------------------
architecture_domain_ans/agent.py          45      3    93%    67-69
architecture_domain_ans/tools/...        120      8    93%    150-152, 200-205
---------------------------------------------------------------------
TOTAL                                    500     40    92%
```

## üîß Configura√ß√£o Avan√ßada

### pytest.ini

O arquivo `pytest.ini` na raiz do projeto cont√©m:

```ini
[pytest]
testpaths = tests bv_ans/testes/unit_tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts = 
    -v
    --strict-markers
    --tb=short
    --cov=architecture_domain_ans
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=85
```

### Vari√°veis de Ambiente

Os testes unit√°rios automaticamente configuram:
- `USE_MOCK=true` - For√ßa modo mock
- `GOOGLE_GENAI_USE_VERTEXAI=False` - Desabilita Vertex AI
- `GOOGLE_CLOUD_PROJECT=test-project-id` - Define projeto de teste

Essas configura√ß√µes s√£o definidas no `conftest.py` atrav√©s da fixture `setup_unit_test_environment`.

## üìù Como Adicionar Novos Testes

### 1. Escolha o Arquivo Correto

- **Testes de l√≥gica do agente**: `test_agent_core.py`
- **Testes de ferramentas**: `test_tools.py`
- **Testes de prompts**: `test_prompts.py`

### 2. Siga o Padr√£o AAA

```python
def test_nome_descritivo_do_teste(self, fixture_name):
    """
    Breve descri√ß√£o do que est√° sendo testado.
    
    Scenario: Contexto do teste
    Expected: Resultado esperado
    """
    # ARRANGE - Preparar dados e mocks
    dados = {"campo": "valor"}
    
    # ACT - Executar a fun√ß√£o
    resultado = funcao_testada(dados)
    
    # ASSERT - Verificar resultados
    assert resultado['sucesso'] is True
    assert resultado['campo'] == 'valor_esperado'
```

### 3. Use Fixtures do conftest.py

```python
def test_com_fixture(self, valid_cnpj, onetrust_data_found):
    """Test usando fixtures pr√©-definidas."""
    resultado = integrar_onetrust(valid_cnpj)
    assert resultado['encontrado'] is True
```

### 4. Adicione Novas Fixtures se Necess√°rio

No `conftest.py`:

```python
@pytest.fixture
def nova_fixture():
    """
    Descri√ß√£o da fixture.
    
    Returns:
        tipo: Descri√ß√£o do retorno
    """
    return {"dados": "teste"}
```

### 5. Use Mocking para Depend√™ncias Externas

```python
from unittest.mock import Mock, patch

def test_com_mock(self):
    """Test usando mock de reposit√≥rio."""
    with patch('modulo.get_repository') as mock_repo:
        mock_instance = Mock()
        mock_instance.get.return_value = {"resultado": "mockado"}
        mock_repo.return_value = mock_instance
        
        resultado = funcao_testada()
        assert resultado is not None
```

### 6. Nomenclatura de Testes

Siga o padr√£o: `test_<funcionalidade>_<cen√°rio>_<resultado_esperado>`

Exemplos:
- `test_agent_initialization_with_valid_config_succeeds`
- `test_tool_execution_with_invalid_input_returns_error`
- `test_prompt_structure_has_minimum_length`

## üéØ Boas Pr√°ticas

### ‚úÖ FA√áA

- **Isole os testes**: Cada teste deve ser independente
- **Use fixtures**: Reutilize dados com fixtures do conftest.py
- **Mock depend√™ncias externas**: APIs, bancos de dados, Vertex AI
- **Teste edge cases**: Inputs vazios, inv√°lidos, extremos
- **Documente testes**: Docstrings claras com Scenario/Expected
- **Mantenha testes r√°pidos**: < 1 segundo por teste unit√°rio
- **Um conceito por teste**: N√£o teste m√∫ltiplas coisas no mesmo teste

### ‚ùå N√ÉO FA√áA

- N√£o dependa da ordem de execu√ß√£o dos testes
- N√£o use valores hardcoded (use fixtures)
- N√£o teste m√∫ltiplas funcionalidades em um teste
- N√£o fa√ßa chamadas reais a APIs externas
- N√£o compartilhe estado entre testes
- N√£o ignore testes falhando (`@pytest.mark.skip` sem raz√£o v√°lida)

## üêõ Troubleshooting

### Erro: "ModuleNotFoundError"

```powershell
# Certifique-se de estar no diret√≥rio raiz do projeto
cd "C:\Users\dajr\OneDrive - GFT Technologies SE\Documents\GFT\BU GCP\Agent Reviewer\repo\arquiteto_parecerista_ans"

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Erro: "Fixture not found"

Verifique se a fixture est√° definida no `conftest.py` do diret√≥rio correto.

### Erro: Cobertura < 85%

1. Execute com `--cov-report=term-missing` para ver linhas n√£o cobertas
2. Adicione testes para as linhas faltantes
3. Considere se algumas linhas podem ser exclu√≠das da cobertura

### Testes Lentos

```powershell
# Identifique testes lentos
pytest bv_ans/testes/unit_tests/ --durations=10

# Marque testes lentos
@pytest.mark.slow
def test_operacao_lenta():
    pass
```

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial

- [Pytest Documentation](https://docs.pytest.org/)
- [pytest-cov Documentation](https://pytest-cov.readthedocs.io/)
- [Google ADK Documentation](https://cloud.google.com/agent-development-kit/docs)

### Comandos √öteis

```powershell
# Ver todos os testes sem executar
pytest bv_ans/testes/unit_tests/ --collect-only

# Executar com output detalhado
pytest bv_ans/testes/unit_tests/ -vv

# Parar no primeiro erro
pytest bv_ans/testes/unit_tests/ -x

# Modo quiet (menos output)
pytest bv_ans/testes/unit_tests/ -q

# Executar testes que falharam na √∫ltima execu√ß√£o
pytest bv_ans/testes/unit_tests/ --lf

# Depurar teste espec√≠fico com PDB
pytest bv_ans/testes/unit_tests/test_agent_core.py::test_nome -vv --pdb
```

## ü§ù Contribuindo

Ao adicionar novos recursos ao agente:

1. **Escreva os testes primeiro** (TDD - Test Driven Development)
2. **Garanta cobertura m√≠nima de 85%** para o novo c√≥digo
3. **Execute toda a su√≠te** antes de fazer commit
4. **Documente casos especiais** nos testes
5. **Atualize este README** se necess√°rio

## üìû Suporte

Para d√∫vidas sobre os testes:

1. Consulte este README
2. Veja exemplos nos arquivos de teste existentes
3. Consulte a documenta√ß√£o do pytest
4. Entre em contato com a equipe de QA

---

**Vers√£o**: 1.0  
**√öltima Atualiza√ß√£o**: Dezembro 2025  
**Autor**: Equipe de Engenharia de Qualidade - Arquiteto Parecerista ANS

