# ğŸ§ª ADK Evaluation - BV ANS Agent

AvaliaÃ§Ã£o local programÃ¡tica usando o framework ADK (Agent Development Kit) da Google.

---

## ğŸ“‹ VisÃ£o Geral

Este mÃ³dulo fornece avaliaÃ§Ã£o rÃ¡pida e local do agente BV ANS, ideal para:
- âœ… Desenvolvimento e debugging
- âœ… Testes em CI/CD pipelines
- âœ… ValidaÃ§Ã£o rÃ¡pida de mudanÃ§as
- âœ… AnÃ¡lise detalhada de casos especÃ­ficos

---

## ğŸš€ InÃ­cio RÃ¡pido

### **PrÃ©-requisitos**

```bash
# Certifique-se de estar no diretÃ³rio correto
cd bv_ans/testes/eval/adk_evaluation

# Verifique se as dependÃªncias estÃ£o instaladas
pip install -r ../../../../requirements.txt
```

### **AvaliaÃ§Ã£o RÃ¡pida (5-7 minutos)**

```bash
python run_quick_eval.py
```

Executa 7 casos de teste representativos cobrindo:
- AnÃ¡lise de documentos
- AnÃ¡lise de planilhas
- ExtraÃ§Ã£o de contratos
- Pareceres rÃ¡pidos
- Tratamento de erros
- Status do sistema

### **AvaliaÃ§Ã£o Completa (15-20 minutos)**

```bash
python run_evaluation.py
```

Executa todos os 15 casos de teste do dataset completo.

---

## ğŸ“Š Casos de Teste

### **Categorias**

| Categoria | Casos | DescriÃ§Ã£o |
|-----------|-------|-----------|
| `document_analysis` | 2 | AnÃ¡lise de especificaÃ§Ãµes tÃ©cnicas e propostas |
| `spreadsheet_analysis` | 2 | AnÃ¡lise de planilhas Excel/CSV |
| `contract_extraction` | 1 | ExtraÃ§Ã£o de dados de contratos |
| `multi_document_analysis` | 1 | ComparaÃ§Ã£o entre mÃºltiplos documentos |
| `simple_opinion` | 1 | Pareceres rÃ¡pidos |
| `system_status` | 1 | VerificaÃ§Ã£o de status |
| `error_handling` | 2 | Tratamento de erros |
| `complex_analysis` | 1 | AnÃ¡lise complexa tÃ©cnica+comercial |
| `performance` | 1 | Testes de performance |
| `edge_case` | 1 | Casos limite |
| `real_world` | 1 | Caso real compliance |
| `integration` | 1 | Teste de integraÃ§Ã£o completa |

### **Visualizar Dataset**

```bash
python dataset.py
```

Output:
```
ğŸ“Š BV ANS Agent Evaluation Dataset Statistics

Total test cases: 15

Categories:
  - document_analysis: 2 tests
  - spreadsheet_analysis: 2 tests
  - contract_extraction: 1 tests
  ...
```

---

## ğŸ“ˆ MÃ©tricas Avaliadas

### **MÃ©tricas BÃ¡sicas**
1. **Success**: Agente completou a tarefa com sucesso?
2. **Response Quality**: Qualidade geral da resposta

### **MÃ©tricas Customizadas** (especÃ­ficas BV ANS)
3. **Document Analysis**: Qualidade de anÃ¡lise de documentos
4. **Spreadsheet Analysis**: PrecisÃ£o em anÃ¡lise de planilhas
5. **Framework Adherence**: AderÃªncia ao framework de 8 pilares
6. **Tool Usage**: Uso correto de ferramentas
7. **Response Completeness**: Completude da resposta
8. **Performance**: EficiÃªncia e tempo de resposta

Cada mÃ©trica retorna:
- **Score**: 0.0 a 1.0 (0% a 100%)
- **Feedback**: ComentÃ¡rios descritivos
- **Details**: Breakdown detalhado

---

## ğŸ“„ Resultados

### **Formato JSON**

Resultados sÃ£o salvos em `results/evaluation_YYYYMMDD_HHMMSS.json`:

```json
{
  "timestamp": "20251207_143022",
  "total_tests": 15,
  "passed": 13,
  "failed": 2,
  "results": [
    {
      "test_id": "TC-DOC-001",
      "scenario": "AnÃ¡lise de EspecificaÃ§Ã£o TÃ©cnica Completa",
      "category": "document_analysis",
      "execution_time": 8.45,
      "iterations": 3,
      "passed": true,
      "evaluation": {
        "overall_score": 0.87,
        "metrics": {
          "success": {"score": 1.0},
          "document_analysis": {"score": 0.92},
          "framework_adherence": {"score": 0.88},
          ...
        }
      }
    }
  ]
}
```

### **RelatÃ³rio HTML**

Dashboard interativo salvo em `results/evaluation_YYYYMMDD_HHMMSS.html`:

- ğŸ“Š Resumo executivo com scores
- âœ… Testes passados/falhados
- ğŸ“ˆ Breakdown por categoria
- ğŸ” Detalhes de cada teste
- ğŸ¨ VisualizaÃ§Ã£o colorida

Abra no navegador para navegaÃ§Ã£o interativa.

---

## ğŸ”§ PersonalizaÃ§Ã£o

### **Executar Casos EspecÃ­ficos**

```python
# run_custom.py
import asyncio
from run_evaluation import AgentEvaluator

async def main():
    evaluator = AgentEvaluator(
        project_id="gft-bu-gcp",
        location="us-central1"
    )
    await evaluator.initialize()
    
    # Executar apenas testes de documento
    await evaluator.run_all_tests(test_ids=[
        "TC-DOC-001",
        "TC-DOC-002"
    ])
    
    evaluator.print_report()
    evaluator.save_results()

asyncio.run(main())
```

### **Executar por Categoria**

```python
# Executar apenas testes de spreadsheet
await evaluator.run_all_tests(categories=["spreadsheet_analysis"])
```

### **Adicionar Novo Caso de Teste**

Edite `dataset.py`:

```python
{
    "test_id": "TC-CUSTOM-001",
    "scenario": "Meu Caso de Teste Customizado",
    "category": "custom",
    "input": {
        "request": {
            "tipo_analise": "custom",
            # ... seus dados
        }
    },
    "expected_output": {
        "sucesso": True,
        # ... expectativas
    },
    "evaluation_criteria": {
        "must_succeed": True,
        # ... critÃ©rios
    }
}
```

---

## ğŸ¯ InterpretaÃ§Ã£o de Scores

| Score Range | ClassificaÃ§Ã£o | AÃ§Ã£o |
|-------------|---------------|------|
| 0.90 - 1.00 | ğŸŸ¢ Excelente | Pronto para produÃ§Ã£o |
| 0.75 - 0.89 | ğŸŸ¡ Bom | Revisar falhas pontuais |
| 0.60 - 0.74 | ğŸŸ  Adequado | Melhorias necessÃ¡rias |
| 0.00 - 0.59 | ğŸ”´ Insuficiente | CorreÃ§Ãµes crÃ­ticas |

### **AnÃ¡lise de Falhas**

Quando um teste falha (score < 0.70):

1. **Verifique o feedback**: Cada mÃ©trica fornece feedback especÃ­fico
2. **Analise a resposta**: Veja `response` no JSON
3. **Compare com esperado**: Veja `expected_output`
4. **Revise os critÃ©rios**: Veja `evaluation_criteria`

---

## ğŸ› Troubleshooting

### **Erro: "Cannot import root_agent"**

```bash
# Verifique o caminho do agente
cd ../../../
python -c "from src.routes.agent import root_agent; print(root_agent)"

# Se falhar, ajuste sys.path em run_evaluation.py
```

### **Testes muito lentos**

```bash
# Use quick eval
python run_quick_eval.py

# Ou reduza o dataset temporariamente em dataset.py
```

### **Errors de encoding (Windows)**

O script jÃ¡ inclui correÃ§Ã£o automÃ¡tica de encoding UTF-8. Se ainda houver problemas:

```bash
# No PowerShell, execute:
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8
python run_evaluation.py
```

### **Vertex AI credentials**

```bash
# Configure credenciais
gcloud auth application-default login

# Ou defina variÃ¡vel de ambiente
$env:GOOGLE_APPLICATION_CREDENTIALS="C:\path\to\key.json"
```

---

## ğŸ“š Estrutura de Arquivos

```
adk_evaluation/
â”œâ”€â”€ __init__.py              # MÃ³dulo Python
â”œâ”€â”€ README.md                # Este arquivo
â”œâ”€â”€ dataset.py               # 15 casos de teste documentados
â”œâ”€â”€ custom_metrics.py        # 6 mÃ©tricas customizadas
â”œâ”€â”€ metrics.py               # Agregador de mÃ©tricas
â”œâ”€â”€ run_evaluation.py        # Script principal (todos os testes)
â”œâ”€â”€ run_quick_eval.py        # Script rÃ¡pido (7 testes)
â””â”€â”€ results/                 # Resultados salvos
    â”œâ”€â”€ .gitkeep
    â”œâ”€â”€ evaluation_*.json    # Resultados detalhados
    â””â”€â”€ evaluation_*.html    # Dashboard interativo
```

---

## ğŸ”„ IntegraÃ§Ã£o CI/CD

### **GitHub Actions**

```yaml
name: Agent Evaluation

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run Quick Evaluation
        env:
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT }}
          GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_SA_KEY }}
        run: |
          cd bv_ans/testes/eval/adk_evaluation
          python run_quick_eval.py
      - name: Upload results
        uses: actions/upload-artifact@v2
        with:
          name: evaluation-results
          path: bv_ans/testes/eval/adk_evaluation/results/
```

### **GitLab CI**

```yaml
evaluate:
  stage: test
  script:
    - pip install -r requirements.txt
    - cd bv_ans/testes/eval/adk_evaluation
    - python run_quick_eval.py
  artifacts:
    paths:
      - bv_ans/testes/eval/adk_evaluation/results/
    expire_in: 1 week
```

---

## ğŸ“ Suporte

**DÃºvidas?**
- Time de Arquitetura: arquitetura@bancobv.com.br
- GFT BU GCP: bucp@gft.com
- DocumentaÃ§Ã£o ADK: https://google.github.io/adk-docs/evaluate/

---

**Desenvolvido com â¤ï¸ pelo Time de Arquitetura - Banco BV & GFT**

