# Evaluation Framework - Architecture Domain ANS Agent

Framework de avalia√ß√£o completo com **duas abordagens independentes e organizadas**.

## üìÅ Estrutura Organizada

```
eval/
‚îú‚îÄ‚îÄ adk_evaluation/              # ‚úÖ Avalia√ß√£o ADK (Local/CI-CD)
‚îÇ   ‚îú‚îÄ‚îÄ run_evaluation.py        # Script completo (8 testes)
‚îÇ   ‚îú‚îÄ‚îÄ run_quick_eval.py        # Script r√°pido (3 testes)
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py               # Dataset de teste
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py               # M√©tricas de avalia√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ custom_metrics.py        # M√©tricas ANS customizadas
‚îÇ   ‚îú‚îÄ‚îÄ results/                 # Resultados JSON locais
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results_*.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report_*.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md                # ‚Üê Documenta√ß√£o completa ADK
‚îÇ
‚îú‚îÄ‚îÄ vertex_ai_evaluation/        # ‚òÅÔ∏è Avalia√ß√£o Vertex AI (Enterprise)
‚îÇ   ‚îú‚îÄ‚îÄ run_vertex_ai_evaluation.py  # Script principal
‚îÇ   ‚îú‚îÄ‚îÄ vertex_ai_evaluation.py      # Classe de servi√ßo
‚îÇ   ‚îú‚îÄ‚îÄ vertex_ai_setup.md           # Guia de configura√ß√£o GCP
‚îÇ   ‚îú‚îÄ‚îÄ requirements_vertex_ai.txt   # Depend√™ncias extras
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                   # Dataset de teste
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                   # M√©tricas de avalia√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ custom_metrics.py            # M√©tricas ANS customizadas
‚îÇ   ‚îú‚îÄ‚îÄ results/                     # Resultados JSON + BigQuery
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vertex_ai_evaluation_*.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md                    # ‚Üê Documenta√ß√£o completa Vertex AI
‚îÇ
‚îú‚îÄ‚îÄ __init__.py                  # Inicializa√ß√£o do m√≥dulo
‚îî‚îÄ‚îÄ README.md                    # Este arquivo (√≠ndice)
```

---

## üéØ Escolhendo o Framework Correto

### üìä **ADK Evaluation** (Recomendado para Dev/CI-CD)

**Quando usar**:
- ‚úÖ Desenvolvimento local e testes r√°pidos
- ‚úÖ Pipelines de CI/CD (GitHub Actions, GitLab)
- ‚úÖ Debugging e valida√ß√£o de mudan√ßas
- ‚úÖ Avalia√ß√£o offline sem custos

**Caracter√≠sticas**:
- ‚ö° **R√°pido**: 3-10 minutos
- üí∞ **Gratuito**: Zero custos
- üéØ **8 m√©tricas customizadas ANS**
- üìÑ **Output**: JSON local + console

**Como usar**:
```bash
cd eval/adk_evaluation
python run_evaluation.py      # Completo (8 testes, ~8min)
python run_quick_eval.py      # R√°pido (3 testes, ~3min)
```

üìñ **Documenta√ß√£o**: [`adk_evaluation/README.md`](./adk_evaluation/README.md)

---

### ‚òÅÔ∏è **Vertex AI Evaluation** (Recomendado para Produ√ß√£o)

**Quando usar**:
- ‚úÖ Avalia√ß√£o de releases em produ√ß√£o
- ‚úÖ Dashboard visual para stakeholders
- ‚úÖ Hist√≥rico e compara√ß√£o de vers√µes
- ‚úÖ Integra√ß√£o com BigQuery e GCS
- ‚úÖ Relat√≥rios executivos

**Caracter√≠sticas**:
- üìä **Dashboard**: Console do Vertex AI
- üìà **Hist√≥rico**: BigQuery autom√°tico
- üîÑ **Compara√ß√£o**: Entre vers√µes
- üí∞ **Pago**: ~$5-10 por avalia√ß√£o

**Como usar**:
```bash
cd eval/vertex_ai_evaluation
python run_vertex_ai_evaluation.py --real         # Execu√ß√£o completa
python run_vertex_ai_evaluation.py --dry-run      # Valida√ß√£o
```

üìñ **Documenta√ß√£o**: [`vertex_ai_evaluation/README.md`](./vertex_ai_evaluation/README.md)

---

## üìä Dataset de Teste

**8 test cases** cobrindo todos os cen√°rios ANS:

| Test ID | Cen√°rio | Categoria |
|---------|---------|-----------|
| TC-001 | Renova√ß√£o - Hist√≥rico Favor√°vel | `renovacao_favoravel` |
| TC-002 | Nova Contrata√ß√£o - Armazena Dados BV | `nova_contratacao_ressalvas` |
| TC-003 | Sistema Legado para Desinvestir | `desinvestimento` |
| TC-004 | Renova√ß√£o - Vencimento > 2 Anos | `vencimento_longo` |
| TC-005 | Renova√ß√£o - Vencimento Ausente (BLOQUEIO) | `bloqueio_critico` |
| TC-006 | Nova Contrata√ß√£o - M√∫ltiplas Integra√ß√µes | `integracao_moderna` |
| TC-007 | Nova Contrata√ß√£o - Fluxo INBOUND | `fluxo_inbound` |
| TC-008 | Renova√ß√£o - Direcionador Manter | `manter` |

---

## üìà M√©tricas de Avalia√ß√£o

**8 m√©tricas customizadas ANS** (implementadas em `metrics.py`):

| M√©trica | Descri√ß√£o | Score |
|---------|-----------|-------|
| **onetrust_validation** | Valida integra√ß√£o com OneTrust | 0.0 - 1.0 |
| **cmdb_validation** | Valida consulta ao CMDB | 0.0 - 1.0 |
| **parecer_suggestion_accuracy** | Precis√£o do parecer sugerido | 0.0 - 1.0 |
| **ressalvas_detection** | Detec√ß√£o de ressalvas necess√°rias | 0.0 - 1.0 |
| **confidence_score_validity** | Validade do score de confian√ßa | 0.0 - 1.0 |
| **alertas_detection** | Detec√ß√£o de alertas | 0.0 - 1.0 |
| **bloqueio_detection** | Detec√ß√£o de bloqueios cr√≠ticos | 0.0 - 1.0 |
| **response_completeness** | Completude da resposta | 0.0 - 1.0 |

**Threshold de Sucesso**: ‚â•0.7 por m√©trica, ‚â•75% pass rate

---

## üéØ Resultados Recentes

### √öltima Avalia√ß√£o (30/11/2025)

```
Framework: Vertex AI Evaluation Service
Version: v1.0
Pass Rate: 87.5% (7/8)
Average Score: 0.93
Status: ‚úÖ EXCELLENT
```

**Detalhes por Test Case**:
- TC-001: 1.00 ‚úÖ PERFECT
- TC-002: 0.96 ‚úÖ EXCELLENT
- TC-003: 0.91 ‚úÖ VERY GOOD
- TC-004: 0.86 ‚úÖ GOOD
- TC-005: 0.62 ‚ö†Ô∏è (bloqueio esperado)
- TC-006: 1.00 ‚úÖ PERFECT
- TC-007: 0.75 ‚úÖ GOOD
- TC-008: 0.88 ‚úÖ VERY GOOD

---

## üîß Configura√ß√£o

### Depend√™ncias ADK
J√° inclu√≠das no `requirements.txt` principal.

### Depend√™ncias Vertex AI
```bash
pip install -r eval/vertex_ai_evaluation/requirements_vertex_ai.txt
```

### Vari√°veis de Ambiente (Vertex AI)
```bash
export PROJECT_ID="gft-bu-gcp"
export LOCATION="us-central1"
```

---

## üìö Documenta√ß√£o Detalhada

- **ADK Evaluation**: [`adk_evaluation/README.md`](./adk_evaluation/README.md)
- **Vertex AI Evaluation**: [`vertex_ai_evaluation/README.md`](./vertex_ai_evaluation/README.md)
- **Vertex AI Setup**: [`vertex_ai_evaluation/vertex_ai_setup.md`](./vertex_ai_evaluation/vertex_ai_setup.md)

---

## ü§ù Workflow Recomendado

### Durante Desenvolvimento
```bash
# Teste r√°pido (3 minutos)
cd eval/adk_evaluation
python run_quick_eval.py
```

### Antes de Commit/PR
```bash
# Teste completo local (8 minutos)
cd eval/adk_evaluation
python run_evaluation.py
```

### Deploy em Produ√ß√£o
```bash
# Avalia√ß√£o completa com dashboard (10 minutos)
cd eval/vertex_ai_evaluation
python run_vertex_ai_evaluation.py --agent-version v1.0 --real
```

---

**√öltima atualiza√ß√£o**: 30 de novembro de 2025  
**Status**: ‚úÖ **ESTRUTURA ORGANIZADA E FUNCIONAL**

# Compare versions
python -m eval.run_vertex_ai_evaluation --compare v1.0 v1.1

# View historical results
python -m eval.run_vertex_ai_evaluation --history --limit 10
```

**üìñ For complete Vertex AI setup and usage, see**: `eval/vertex_ai_setup.md`

### Specific Tests

Run specific test cases:

```python
from eval.run_evaluation import AgentEvaluator
import asyncio

async def run():
    evaluator = AgentEvaluator(project_id="your-project", location="global")
    await evaluator.initialize()
    await evaluator.run_all_tests(test_ids=["TC-001", "TC-005"])
    evaluator.print_report()

asyncio.run(run())
```

## üìä Results

Results are saved to `eval/results/` directory:

- `evaluation_results_YYYYMMDD_HHMMSS.json`: Detailed results with full responses
- `evaluation_report_YYYYMMDD_HHMMSS.json`: Summary report with metrics

### Example Report

```json
{
  "summary": {
    "total_tests": 8,
    "passed": 7,
    "failed": 1,
    "errors": 0,
    "pass_rate": "87.5%",
    "average_execution_time_seconds": "45.32"
  },
  "metric_averages": {
    "onetrust_validation": "100.00%",
    "cmdb_validation": "100.00%",
    "parecer_suggestion_accuracy": "87.50%",
    "confidence_score_validity": "100.00%",
    "...": "..."
  }
}
```

## üîß Configuration

Set environment variables in `.env`:

```bash
# GCP Configuration
GOOGLE_CLOUD_PROJECT=gft-bu-gcp
GOOGLE_CLOUD_LOCATION=global

# Optional: Save quick eval results
SAVE_QUICK_EVAL_RESULTS=false
```

## ü§ñ CI/CD Integration

See `github-actions-example.yml` for GitHub Actions integration.

### Example Workflow

```yaml
name: Evaluate Agent
on: [push, pull_request]
jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Quick Evaluation
        run: python -m eval.run_quick_eval
```

## üìù Adding New Test Cases

To add a new test case, edit `dataset.py`:

```python
{
    "test_id": "TC-009",
    "scenario": "Your Test Scenario",
    "category": "your_category",
    "input": {
        "solicitante": {...},
        "requisicao": {...}
    },
    "expected_output": {
        "sucesso": True,
        "parecer_sugerido": "Parecer Favor√°vel",
        ...
    },
    "evaluation_criteria": {
        "must_succeed": True,
        ...
    }
}
```

## üìö ADK Documentation

Based on: https://google.github.io/adk-docs/evaluate/

## üéØ Passing Criteria

A test case **passes** if:
- Average metric score ‚â• 0.7 (70%)
- No critical errors
- All mandatory metrics pass

## üêõ Troubleshooting

### Common Issues

1. **Import Error**: Ensure you're running from the agent root directory
2. **API Errors**: Check GCP credentials and project permissions
3. **Mock Data**: Ensure mock adapters are properly configured

### Debug Mode

Enable detailed logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üìÑ License

Same as parent project.

---

**Last Updated**: 2025-11-29  
**Version**: 1.0  
**Maintainer**: Architecture Domain ANS Team

