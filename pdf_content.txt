
--- Page 1 ---

Guia Técnico de Migração: Arquitetando a
Transição do GPT para o Gemini 3.0 Pro
1. Sumário Executivo: A Ascensão dos Motores de Raciocínio
Este documento constitui uma análise técnica exaustiva e uma proposta de migração desenhada
especificamente para líderes técnicos, arquitetos de soluções e engenheiros de backend que operam
atualmente workloads de produção baseados em modelos OpenAI (GPT-4, GPT-4 Turbo, GPT-4o) e buscam
transicionar para o ecossistema Google Cloud utilizando o modelo Gemini 3.0 Pro.
A indústria de Inteligência Artificial Generativa está atravessando um ponto de inflexão arquitetural crítico.
Estamos migrando de uma era dominada por Modelos de Linguagem (LLMs) estatísticos, que simulam o
raciocínio através da previsão de tokens sequenciais baseada em engenharia de prompt intensiva (como
Chain-of-Thought), para uma nova geração de Motores de Raciocínio (Reasoning Engines). O Gemini 3.0
Pro exemplifica essa mudança através de sua arquitetura "Action over Thought", onde o processo cognitivo é
desacoplado da geração de resposta, internalizado em um orçamento computacional opaco e controlável.
Para as equipes de engenharia, esta migração não é uma mera substituição de endpoints de API
(api.openai.com para aiplatform.googleapis.com). Ela exige uma reestruturação fundamental da camada de
interação da aplicação para acomodar novos primitivos como Thought Signatures (Assinaturas de
Pensamento), protocolos estritos de Function Calling e estratégias econômicas de Context Caching. A
simples "tradução" de prompts do GPT-4 para o Gemini resultará em subutilização das capacidades do
modelo e falhas de raciocínio. Uma abordagem "Bare Metal", que compreenda a estrutura da requisição
REST e o ciclo de vida do estado cognitivo do modelo, é mandatória.
Este relatório, estruturado sob o framework de adoção "Três As" (Avaliar, Adaptar, Aumentar), detalha os
passos rigorosos necessários para re-arquitetar suas aplicações. Cobriremos desde a eliminação da
temperatura como variável de controle em modelos de raciocínio até a implementação de pipelines de
otimização algorítmica de prompts (VAPO), culminando em uma estratégia de cache que transforma a
economia de escala de aplicações baseadas em RAG (Retrieval-Augmented Generation).


--- Page 2 ---

2. Mudança de Paradigma: "Action over Thought" e Thinking
Levels
O ponto de atrito mais significativo na migração do ecossistema OpenAI para o Gemini 3.0 é a necessidade
de "desaprender" práticas arraigadas de Engenharia de Prompt em favor de uma "Configuração de
Raciocínio". No ecossistema GPT, a lógica complexa é extraída através de instruções verbosas e técnicas de
few-shot prompting que forçam o modelo a "pensar em voz alta". O Gemini 3.0 internaliza esse processo.
2.1 Desconstruindo o thinking_level
O parâmetro thinking_level não é uma garantia de tokens; é uma concessão relativa de profundidade
computacional.1 Diferente dos modelos anteriores que geram tokens de forma linear e probabilística
imediata, o Gemini 3.0, quando configurado com níveis elevados de pensamento, entra em um estado de
processamento latente antes de emitir o primeiro token.


--- Page 3 ---

A documentação técnica da Vertex AI define dois estados primários operacionais para este parâmetro 2:
2.1.1 thinking_level="low" (Otimizado para Latência)
● Comportamento Técnico: O modelo minimiza o Time-To-First-Token (TTFT). O processo de verificação
interna (vetting) é reduzido ao mínimo essencial. O modelo opera de forma reativa, confiando em
padrões superficiais aprendidos durante o pré-treinamento.
● Cenários de Uso: Chatbots de alta vazão (high-throughput), tarefas simples de extração de entidades,
classificação de texto, tradução direta e seguimento de instruções simples.
● Equivalência GPT: Comparável ao GPT-3.5-Turbo ou GPT-4-Turbo com temperatura próxima a 0,
focado em velocidade.
● Implicação de Custo: Menor consumo de recursos computacionais, resultando em menor latência e
custo por requisição.
2.1.2 thinking_level="high" (Otimizado para Raciocínio - Padrão)
● Comportamento Técnico: O modelo pode pausar significativamente (de segundos a dezenas de
segundos) antes de gerar a saída. Durante esse silêncio, o modelo não está ocioso; ele está
atravessando um espaço de busca de caminhos lógicos potenciais, verificando fatos contra sua base de
conhecimento parametrizada e estruturando saídas complexas. O output é gerado somente após o
modelo atingir um nível de confiança interno na integridade do raciocínio.
● Cenários de Uso: Análise jurídica contratual, geração de código complexo e arquitetura de software,
planejamento de múltiplos passos, análise de causalidade em dados não estruturados.
● Equivalência GPT: Comparável ao GPT-4 ou GPT-4o utilizando técnicas extensivas de
Chain-of-Thought (CoT), porém com uma redução drástica na verbosidade do prompt e menor taxa de
alucinação.1
Ação Crítica de Migração:
Ao migrar, remova instruções de CoT dos seus prompts. Frases como "Pense passo a passo", "Explique seu
raciocínio antes de responder" ou "Quebre isso em etapas menores" tornam-se redundantes e podem
interferir negativamente no processo de pensamento nativo do modelo (System 2 thinking). Mapeie a
complexidade da tarefa diretamente para o parâmetro thinking_level.
2.2 O Fim da temperature em Modelos de Raciocínio
No stack da OpenAI, a manipulação da temperature é uma ferramenta padrão para controlar a criatividade
versus determinismo (ex: 0.0 para código, 0.7 para escrita criativa). Para os modelos de raciocínio do Gemini
3.0, essa prática deve ser abandonada.
A documentação da Vertex AI é explícita e contundente: "Se o seu código existente define explicitamente
a temperatura (especialmente para valores baixos visando saídas determinísticas), recomendamos


--- Page 4 ---

remover este parâmetro e usar o padrão do Gemini 3 de 1.0".1
Análise de Causa Raiz:
A temperatura controla a entropia da distribuição de probabilidade do próximo token. Em um modelo
"Thinking", o processo de raciocínio gera tokens internos (pensamentos) que não são visíveis ao usuário final,
mas que são cruciais para chegar à resposta correta. Ao definir uma temperatura baixa (ex: 0.1), você está
artificialmente restringindo a capacidade do modelo de explorar ramificações lógicas complexas durante seu
processo de pensamento oculto. Isso colapsa a árvore de busca do modelo, levando a loops repetitivos,
degradação de performance em tarefas complexas e, paradoxalmente, respostas menos precisas.
Regra de Migração:
● Configuração Legada (GPT): temperature=0.0, top_p=0.1
● Configuração Nova (Gemini 3.0): thinking_level="high", temperature=1.0 (ou omitir o parâmetro).
Deixe o motor de inferência gerenciar a entropia necessária para o raciocínio.
3. Integração "Bare Metal": Vertex AI REST & Estrutura Estrita
Para arquitetos de soluções que gerenciam gateways de API de alto desempenho, depender exclusivamente
de SDKs de alto nível pode ocultar detalhes críticos do protocolo de comunicação. Uma compreensão "Bare
Metal" da estrutura JSON da API Vertex AI é essencial para depuração, logging avançado e implementação
de middlewares de resiliência.
A migração da API da OpenAI para a Vertex AI REST exige uma reescrita completa da camada de serialização
de rede. Não existe compatibilidade direta nos payloads.
3.1 Anatomia do Payload: messages vs. contents
A diferença fundamental reside na abstração da estrutura da mensagem. A OpenAI utiliza um array linear de
messages. A Vertex AI utiliza um array de contents, onde cada item possui role e parts. Essa abstração parts
é crítica pois o Gemini é nativamente multimodal; uma única mensagem de turno pode conter texto, binários
de imagem e chamadas de função como "partes" distintas, porém atômicas, da mesma interação.
Requisição OpenAI (Modelo Conceitual)
JSON


--- Page 5 ---

{
"model": "gpt-4",
"messages": [
{"role": "system", "content": "Você é um assistente útil."},
{"role": "user", "content": "Verifique o estoque do item X"}
],
"tools": [...],
"temperature": 0.2
}
Requisição Vertex AI Gemini 3.0 (Modelo Conceitual)
Observe a estrutura aninhada de parts e a ausência de temperatura baixa.
JSON
{
"contents": [
{
"role": "user",
"parts": [
{"text": "Verifique o estoque do item X"}
]
}
],
"tools": [...],
"generation_config": {
"thinking_level": "high",
"temperature": 1.0
},
"system_instruction": {
"parts": [
{"text": "Você é um assistente útil."}
]
}
}
Nota Técnica: A instrução de sistema (system_instruction) é movida para fora do array de contents histórico,
tornando-se um parâmetro de configuração de topo.3


--- Page 6 ---

3.2 A Arquitetura functionCall e Thought Signatures
Este é o diferenciador técnico mais crítico e a maior fonte de erros durante a migração. No Gemini 3.0, o
raciocínio é um processo com estado (stateful) que deve ser preservado pelo cliente.
Quando o Gemini 3.0 decide invocar uma ferramenta, ele gera dois artefatos críticos na resposta 1:
1. functionCall: O objeto JSON padrão contendo o nome da função e os argumentos validados.
2. thoughtSignature: Uma string opaca (token criptográfico ou hash de estado) que representa o
caminho de raciocínio que levou àquela decisão.
A Restrição de Protocolo: Você DEVE retornar a thoughtSignature na chamada de API subsequente
(quando devolver o resultado da função). Se você falhar em passar esta assinatura de volta, o modelo sofre
uma "amnésia de raciocínio": ele recebe o resultado da função, mas perde o contexto cognitivo do porquê
ele solicitou aquela execução, quebrando a cadeia de raciocínio lógico.
Guia de Implementação "Bare Metal" (Python Pattern)
Abaixo apresento um padrão de implementação em Python que utiliza a estrutura do SDK da Vertex AI, mas
ilustra explicitamente a lógica "raw" necessária para manipular o loop de resposta e a preservação da


--- Page 7 ---

thoughtSignature.
Python
# PADRÃO DE MIGRAÇÃO: Manipulação de Thought Signatures e Function Calling Estrito
# Referência Técnica:
import vertexai
from vertexai.generative_models import GenerativeModel, Tool, Part, Content
import vertexai.preview.generative_models as preview_genai
def execute_reasoning_turn(chat_session, user_prompt):
"""
Executa um turno com Gemini 3.0, manipulando function calling estrito
e a persistência obrigatória da thought signature.
"""
# 1. Enviar Requisição Inicial
# Nota: 'thinking_level' é definido na generation_config durante o init do modelo
response = chat_session.send_message(user_prompt)
# A resposta pode conter múltiplas partes, mas focamos na primeira candidata
# Em produção, itere sobre parts para suportar chamadas paralelas
try:
part = response.candidates.content.parts
except IndexError:
return "Erro: Resposta vazia do modelo."
# 2. Verificar se há uma chamada de função (Function Call)
if part.function_call:
function_name = part.function_call.name
args = part.function_call.args
# CRÍTICO: Extrair a Thought Signature
# Diferente do GPT, precisamos capturar este token opaco.
# Se a SDK abstrair isso, verifique os metadados da resposta.
# Em chamadas REST puras, isso vem no campo "thoughtSignature".
thought_signature = getattr(part, "thought_signature", None)
print(f" Model Thought Signature Capturada: {str(thought_signature)[:20]}...")
# 3. Executar a Ferramenta (Simulação de lógica de negócio)
# Aqui você invocaria sua API real baseada no 'function_name'
tool_result = execute_tool_logic(function_name, args)


--- Page 8 ---

# 4. Construir o Payload de Retorno (Follow-up)
# Devemos retornar DUAS partes lógicas no turno do usuário:
# Parte A: O contexto da chamada original (Function Call + Signature)
# Parte B: A resposta da função (Function Response)
# A SDK da Vertex AI gerencia o histórico, mas se você estiver
# construindo o JSON manualmente (REST), você deve reinjetar a signature.
# Construindo a resposta da função
response_part = Part.from_function_response(
name=function_name,
response={"result": tool_result}
)
# Ao enviar a resposta, o modelo usa o histórico da sessão para conectar
# a resposta à assinatura anterior. Em implementações stateless (REST puro),
# você deve reenviar a estrutura completa do histórico.
final_response = chat_session.send_message(response_part)
return final_response.text
return response.text
def execute_tool_logic(name, args):
# Implementação da lógica da ferramenta "Bare Metal"
# Validação de argumentos deve ocorrer aqui também
if name == "check_inventory":
return {"status": "available", "quantity": 42}
return {"error": "Function not found"}
3.3 Schema JSON & Validação Estrita
O Gemini 3.0 impõe uma validação muito mais rigorosa nas declarações de funções (Function Declarations)
em comparação com o schema tools permissivo da OpenAI. Isso é uma característica de design, não um bug,
visando reduzir alucinações na invocação de ferramentas.
Diferenças Chave e Armadilhas de Migração 5:
● Proibição de $ em Referências: Diferente do OpenAPI schema padrão suportado pelo GPT, no Vertex
AI você deve especificar ref e defs sem o símbolo $ (ex: use ref: "minhaDefinicao" em vez de $ref:
"#/definitions/minhaDefinicao"). A presença do $ causará erros de validação 400 Bad Request.
● Limites de Profundidade e Recursão: A profundidade máxima de aninhamento de objetos no schema
é 32. A profundidade de recursão em defs (autorreferência) é limitada estritamente a 2. Schemas de
objetos de dados complexos e profundamente aninhados que funcionavam no GPT podem falhar aqui.
● Obrigatoriedade Funcional de Descrições: Embora tecnicamente opcionais em alguns validadores


--- Page 9 ---

JSON, as descrições (description) dos campos são funcionalmente mandatórias para que o motor de
raciocínio entenda a semântica. Omissão de descrições em parâmetros resulta em degradação severa
da performance de thinking.
4. Estratégia de Migração de Prompts: Vertex AI Prompt
Optimizer e ReAct
Migrar prompts manualmente é um processo propenso a erros e ineficiente. Prompts desenvolvidos para
GPT-4 frequentemente carregam "dívida técnica" na forma de correções comportamentais ("hacks") que são
desnecessários ou confusos para o Gemini 3.0. Além disso, o padrão ReAct (Reasoning + Acting), que
desenvolvedores implementavam manualmente via prompt engineering no GPT ("Pense, depois aja"), é
obsoleto no Gemini 3.0, pois o modelo o executa nativamente.
A estratégia recomendada é utilizar o Vertex AI Prompt Optimizer (VAPO) para automatizar a reescrita e
adaptação dos prompts.
4.1 O Mecanismo VAPO (Vertex AI Prompt Optimizer)
O VAPO utiliza um loop iterativo de "Otimização por Avaliação" baseado em algoritmos de busca de prompt
(como Automatic Prompt Optimization - APO). Ele trata seu prompt como um hiperparâmetro a ser ajustado
matematicamente contra um modelo alvo específico (neste caso, gemini-3.0-pro).3
O Fluxo de Otimização:
1. Input: Seu System Prompt atual do GPT-4 + Um conjunto de dados de Inputs/Outputs Ideais (Ground
Truth).
2. Processo: O VAPO reescreve o prompt, gera respostas usando o Gemini, avalia essas respostas contra
os outputs ideais (ou usando um LLM-as-a-Judge para critérios subjetivos) e itera para maximizar a
pontuação de avaliação.
3. Output: Um prompt matematicamente otimizado para a arquitetura do Gemini 3.0, frequentemente
removendo verbosidade desnecessária e focando em restrições estruturais.
4.2 Workflow de Migração: A Abordagem "Golden Dataset"
Não copie e cole prompts. Siga este caminho de migração algorítmico:
1. Extração de Traces de Alto Valor: Extraia 50 a 100 pares de requisição/resposta bem-sucedidos dos
seus logs de produção do GPT-4. Estes servirão como sua "Verdade Fundamental" (Golden Dataset).
2. Sanitização: Remova artefatos específicos do GPT das respostas ideais (ex: frases de preâmbulo como


--- Page 10 ---

"Claro, eu posso ajudar com isso...").
3. Execução do VAPO: Configure o job de otimização para converter a intenção do prompt GPT em
instruções Gemini.
4.3 SDK Python: Executando um Job de Otimização
O código a seguir demonstra como submeter um job de otimização de prompt programaticamente. Esta é a
fase "Adaptar" do framework. Observe o uso da classe PromptOptimizerVAPOConfig.7
Python
from vertexai.preview import prompt_optimizer as vapo
from vertexai import init
import logging


--- Page 11 ---

# Inicializar Vertex AI
# Substitua pelo seu ID de projeto e localização (VAPO disponível em us-central1 durante preview)
init(project="seu-project-id", location="us-central1")
# Definir o Alvo: Gemini 3.0 Pro
# Nota: Alvos de otimização são específicos. Garanta que 'gemini-3.0-pro-001'
# esteja listado como target suportado no serviço VAPO da sua região.
target_model = vapo.OptimizationTarget.GEMINI_PRO_3_0
# Configurar a Otimização
# Usamos uma estratégia de maximização em métrica customizada ou padrão
config = vapo.OptimizerConfig(
optimization_target=target_model,
num_steps=10, # Número de iterações de reescrita
num_prompts_per_step=3 # Variantes geradas por passo
)
# O Prompt "Legado" do GPT-4 (Input)
gpt_prompt_legado = """
Você é um assistente de codificação especialista em Python.
Pense passo a passo.
Certifique-se de verificar ponteiros nulos.
Se não souber, diga "Eu não sei".
... (Outras instruções legadas do GPT)...
"""
# Executar Otimização
# 'seus_dados_exemplo' seriam seus logs extraídos do GPT-4 (Pares Input/Output)
# O método retorna o prompt reescrito que maximiza a aderência aos outputs.
try:
logging.info("Iniciando Job de Otimização VAPO...")
response = vapo.optimize_prompt(
prompt=gpt_prompt_legado,
target_model=target_model,
config=config,
# data=seus_dados_exemplo <-- Seu "Golden Dataset" aqui
)
print(f"Prompt Otimizado Vencedor: {response.best_prompt}")
except Exception as e:
logging.error(f"Falha na otimização: {str(e)}")
7
5. Engenharia de Performance: Otimização de Custo e Latência
com Context Caching


--- Page 12 ---

Para aplicações de alto tráfego, o tamanho massivo da janela de contexto do Gemini (até 2M tokens)
apresenta um desafio de custo operacional se não gerenciado. Context Caching é a solução arquitetural
para este problema, oferecendo um mecanismo para reduzir custos em até 90% para tokens de entrada
repetidos e diminuir drasticamente a latência.
5.1 A Economia do Caching na Vertex AI
O modelo de precificação do Gemini na Vertex AI introduz uma nova variável econômica: o custo de
armazenamento de estado.
● Caching Implícito (Gemini 2.5+): Automático e sem esforço. Se o prefixo do seu prompt corresponder
a uma requisição recente no mesmo shard de computação (mínimo 1024-2048 tokens), você recebe um
desconto. Não requer alteração de código, mas não oferece garantias de retenção.10
● Caching Explícito (O Padrão Enterprise): Você cria manualmente uma entrada de cache (um handle
para o KV Cache do modelo). Isso é obrigatório para aplicações que exigem garantias de performance e
possuem contextos pesados e estáveis (ex: base de conhecimento de 500 páginas, documentação de
API completa).
Fórmula de Custo Total de Propriedade (TCO):
$$Custo = (Tokens_{Criação} \times Preço_{Input}) + (Duração_{Armazenamento} \times Preço_{Storage}) +
(Tokens_{Query} \times Preço_{Input\_Cacheado})$$
Onde $Preço_{Input\_Cacheado}$ é tipicamente uma fração (~10-25%) do preço de input padrão. A chave
para o ROI positivo é o parâmetro TTL (Time-To-Live).
5.2 Implementação: Definindo o TTL e Criando o Cache
Gerenciar o TTL é uma arte. Um TTL curto desperdiça custos de criação (reprocessamento dos tokens
iniciais); um TTL excessivamente longo desperdiça custos de armazenamento (aluguel de RAM na TPU). O
ponto ideal (sweet spot) depende da sua Frequência de Consultas (QPS).
Implementação Python: Criando um Contexto com Cache Explícito
Python
import vertexai
from vertexai.preview import caching


--- Page 13 ---

from vertexai.generative_models import Part, GenerativeModel
import datetime
# Inicialização
vertexai.init(project="ai-migration-hub", location="us-central1")
# 1. Definir Conteúdo Estático (Ex: Documentação de API Massiva, Base de Código)
# Este é o conteúdo que consumiria 50k tokens por requisição no GPT-4
system_instruction_texto = """
Você é um Arquiteto de Soluções Sênior.
Responda baseando-se APENAS na Documentação de API fornecida no contexto.
"""
# Carregar contexto pesado (simulado via URI do Google Cloud Storage)
# O Gemini acessa o arquivo diretamente no GCS, evitando upload de bytes na chamada
api_docs_content = [
Part.from_uri(
uri="gs://meu-bucket-seguro/gemini_3_migration_docs_vFinal.pdf",
mime_type="application/pdf"
)
]
# 2. Criar o Cache com TTL Estrito
# Definimos um TTL de 60 minutos. Se as consultas ocorrem a cada minuto,
# economizamos ~90% nos custos de input após a primeira criação.
cached_content = caching.CachedContent.create(
model_name="gemini-3.0-pro-preview-001",
system_instruction=system_instruction_texto,
contents=api_docs_content,
ttl=datetime.timedelta(minutes=60), # <--- A Alavanca de Controle de Custo
display_name="api_docs_cache_v1_prod"
)
print(f"Cache Criado com Sucesso. ID Resource: {cached_content.name}")
# 3. Uso: Referenciando o Cache na Inferência
# Instanciamos o modelo APONTANDO para o conteúdo cacheado.
# O modelo já "sabe" o conteúdo do PDF.
model = GenerativeModel.from_cached_content(cached_content=cached_content)
# Esta requisição paga taxas reduzidas de "Cached Input", não "Input" cheio
response = model.generate_content("Como implemento 'functionCall' segundo a doc?")
print(response.text)
# Opcional: Atualizar o TTL se o cache estiver sendo muito usado (Keep-Alive)
cached_content.update(ttl=datetime.timedelta(minutes=120))
12


--- Page 14 ---

5.3 Otimização de Latência via Arquitetura Híbrida
Enquanto o Caching otimiza a latência de Input (leitura), o thinking_level dita a latência de Processamento
(geração).
● Cenário A (Chatbot Interativo): Use thinking_level="low". O modelo responde instantaneamente, ideal
para saudações ou perguntas triviais.
● Cenário B (Gerador de Relatórios/Análise): Use thinking_level="high". O modelo "pensa" por 10-20
segundos. O usuário deve ser notificado visualmente ("Analisando dados complexos...").
Estratégia Híbrida Recomendada: Implemente uma Arquitetura em Camadas (Tiered Architecture).
1. Roteador (Router): Um modelo pequeno e ultra-rápido (Gemini 2.5 Flash) recebe o prompt e avalia a
complexidade (Score 1-10).
2. Worker:
○ Se complexidade < 5: Roteie para Gemini 3.0 (thinking_level="low") ou mantenha no Flash.
○ Se complexidade >= 5: Roteie para Gemini 3.0 (thinking_level="high").


--- Page 15 ---

6. Framework de Adoção: Os "Três As"
Para garantir uma migração segura de um workload GPT de produção, evitando regressões funcionais,
recomendo seguir este framework faseado.
Fase 1: AVALIAR (Semanas 1-2)
● Auditoria de Prompts: Catalogue todos os prompts em uso. Classifique-os por "Densidade de
Raciocínio" (Baixa/Alta). Prompts de baixa densidade podem ir para o Gemini Flash; alta densidade vão
para o Gemini 3.0 Pro.
● Baseline de Métricas: Registre a latência (P95 e P99) e o custo por turno do seu setup atual GPT-4.
Isso é essencial para provar o sucesso da migração.
● Coleta de Dados: Extraia o "Golden Dataset" (100 interações ideais) para alimentar o VAPO.
Fase 2: ADAPTAR (Semanas 3-4)
● Camada de Rede: Implemente o cliente Vertex AI SDK ou REST. Crucial: Escreva os testes unitários
para a persistência da thoughtSignature. Se a assinatura for perdida, o teste deve falhar.
● Refatoração de Schemas: Valide todos os schemas de ferramentas contra as regras estritas (sem $,
limite de profundidade).
● Tuning de Prompts: Execute jobs do VAPO nos prompts de Alta Densidade de Raciocínio.
Fase 3: AUMENTAR (Semana 5+)
● Ativar Caching: Identifique contextos estáticos (Docs, pedaços RAG > 20k tokens) e implemente
CachedContent com TTLs ajustados à sua curva de tráfego.
● Ligar o Thinking: Mude thinking_level para high nas tarefas onde o GPT-4 alucinava ou exigia muitos
retries.
● Limpeza: Remova instruções legadas de CoT de todos os prompts. Deixe o modelo pensar por conta
própria.
7. Conclusão e Próximos Passos


--- Page 16 ---

A migração para o Gemini 3.0 Pro representa uma transição da escritura de inteligência (via prompt
engineering manual) para o direcionamento de inteligência (via configuração de motores de raciocínio). A
remoção da "muleta" do Chain-of-Thought manual, substituída pelo orçamento de thinking_level, oferece
uma arquitetura mais limpa e robusta para resolução de problemas complexos. No entanto, esse poder vem
atrelado a requisitos de protocolo estritos — especificamente o gerenciamento de Thought Signatures e
schemas REST validados.
Ao alavancar o Vertex AI Prompt Optimizer para preencher a lacuna de adaptação de prompts e o Context
Caching para resolver a equação de custo/latência em escala, as equipes de engenharia podem construir
aplicações que não são apenas mais eficientes financeiramente, mas fundamentalmente mais capazes de
raciocínio autônomo e agêntico.
Item de Ação Imediato: Inicie sua fase de Avaliação hoje isolando suas tarefas de raciocínio mais complexas
e executando-as em um ambiente de teste com thinking_level="high". A diferença na precisão "first-shot"
(acerto na primeira tentativa) será o caso de negócios definitivo para sua migração.
Referências citadas
1. Gemini 3 Developer Guide | Gemini API - Google AI for Developers, acessado em novembro 23,
2025, https://ai.google.dev/gemini-api/docs/gemini-3
2. Get started with Gemini 3 | Generative AI on Vertex AI | Google Cloud Documentation,
acessado em novembro 23, 2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/get-started-with-gemini-3
3. Data-driven prompt optimizer | Generative AI on Vertex AI - Google Cloud Documentation,
acessado em novembro 23, 2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/data-driven-optimize
r
4. Vertex AI SDK migration guide - Google Cloud Documentation, acessado em novembro 23,
2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk
5. Introduction to function calling | Generative AI on Vertex AI - Google Cloud Documentation,
acessado em novembro 23, 2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling
6. Enhance your prompts with Vertex AI Prompt Optimizer - Google Developers Blog, acessado
em novembro 23, 2025,
https://developers.googleblog.com/en/enhance-your-prompts-with-vertex-ai-prompt-optimize
r/
7. Vertex Generative AI SDK for Python bookmark_border - Google Cloud Documentation,
acessado em novembro 23, 2025,
https://docs.cloud.google.com/python/docs/reference/vertexai/latest
8. Zero-shot optimizer | Generative AI on Vertex AI - Google Cloud Documentation, acessado em
novembro 23, 2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/zero-shot-optimizer
9. googleapis/python-aiplatform: A Python SDK for Vertex AI, a fully managed, end-to-end
platform for data science and machine learning. - GitHub, acessado em novembro 23, 2025,
https://github.com/googleapis/python-aiplatform
10. Context caching | Gemini API | Google AI for Developers, acessado em novembro 23, 2025,
https://ai.google.dev/gemini-api/docs/caching


--- Page 17 ---

11. Context caching overview | Generative AI on Vertex AI - Google Cloud Documentation,
acessado em novembro 23, 2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-over
view
12. Create a context cache | Generative AI on Vertex AI - Google Cloud Documentation, acessado
em novembro 23, 2025,
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-creat
e
13. Vertex AI Context Caching with Gemini | by Sascha Heyer | Google Cloud - Medium, acessado
em novembro 23, 2025,
https://medium.com/google-cloud/vertex-ai-context-caching-with-gemini-189117418b67
14. Practical Guide: Using Gemini Context Caching with Large Codebases | by Olejniczak Lukasz |
Google Cloud - Medium, acessado em novembro 23, 2025,
https://medium.com/google-cloud/practical-guide-using-gemini-context-caching-with-large-c
odebases-08d46d946c3d

